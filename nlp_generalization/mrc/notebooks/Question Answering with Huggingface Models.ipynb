{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b974de5",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c62ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForQuestionAnswering were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = \"facebook/opt-350m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b9c4a",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a510ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe6634",
   "metadata": {},
   "source": [
    "#### Remove samples with length larger than model max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35f16b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a35c2c149a419fa8ab8160b1ddd7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25722 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638846348b6d40e3be781e659e0c2a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_function(examples):\n",
    "    question = examples[\"question\"]\n",
    "    context = examples[\"context\"]\n",
    "    input_len = len(tokenizer(question,context, truncation=False).input_ids)\n",
    "    return (input_len < tokenizer.model_max_length)\n",
    "\n",
    "filtered_dataset = squad_dataset.filter(filter_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3dfe27",
   "metadata": {},
   "source": [
    "### Processing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7112c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aac8fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a35eeff27e465b9e8f248a44b0d90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/87598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/varu/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/varu/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 1377, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/home/varu/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3466, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"/home/varu/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3345, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/tmp/ipykernel_1986520/2603310836.py\", line 3, in preprocess_function\n    inputs = tokenizer(\n  File \"/home/varu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2798, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/home/varu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2884, in _call_one\n    return self.batch_encode_plus(\n  File \"/home/varu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3075, in batch_encode_plus\n    return self._batch_encode_plus(\n  File \"/home/varu/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py\", line 788, in _batch_encode_plus\n    raise NotImplementedError(\nNotImplementedError: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mfiltered_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocess_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiltered_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:591\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    590\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 591\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    592\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m    594\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:556\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    549\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    550\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    554\u001B[0m }\n\u001B[1;32m    555\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 556\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    557\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    558\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3181\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3174\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpawning \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_proc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m processes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3175\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[1;32m   3176\u001B[0m     disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[1;32m   3177\u001B[0m     unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3178\u001B[0m     total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[1;32m   3179\u001B[0m     desc\u001B[38;5;241m=\u001B[39m(desc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (num_proc=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_proc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3180\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m iflatmap_unordered(\n\u001B[1;32m   3182\u001B[0m         pool, Dataset\u001B[38;5;241m.\u001B[39m_map_single, kwargs_iterable\u001B[38;5;241m=\u001B[39mkwargs_per_job\n\u001B[1;32m   3183\u001B[0m     ):\n\u001B[1;32m   3184\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   3185\u001B[0m             shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:1417\u001B[0m, in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m   1414\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1415\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pool_changed:\n\u001B[1;32m   1416\u001B[0m         \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[0;32m-> 1417\u001B[0m         [async_result\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:1417\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1414\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1415\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pool_changed:\n\u001B[1;32m   1416\u001B[0m         \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[0;32m-> 1417\u001B[0m         [\u001B[43masync_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.05\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/multiprocess/pool.py:774\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n\u001B[1;32m    773\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 774\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast."
     ]
    }
   ],
   "source": [
    "train_dataset = filtered_dataset[\"train\"].map(preprocess_function, batched=True, num_proc=4, remove_columns=filtered_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876c46a",
   "metadata": {},
   "source": [
    "### Processing the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bef3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "    \n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55746c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = filtered_dataset[\"validation\"].map(preprocess_validation_examples, batched=True, num_proc=4,remove_columns=filtered_dataset[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b948e77",
   "metadata": {},
   "source": [
    "### Prepare Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a87874",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "save_path: str=\"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/nlp-gen\"\n",
    "name = model_name.split(\"/\")[-1]\n",
    "save_path = f\"{save_path}/{name}-squad\"\n",
    "batch_size = 32\n",
    "num_train_epochs = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy = \"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1034ca6",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15208cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, filtered_dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_metrics(start_logits, end_logits, validation_dataset, filtered_dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e35b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9551f",
   "metadata": {},
   "source": [
    "### Evalutate on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_data():\n",
    "        \n",
    "    test_datasets = ['squad','newsqa','sagnikrayc/adversarial_hotpotqa']\n",
    "    dataset2split = {'snli':\"validation\", 'newsqa':\"test\", 'sagnikrayc/adversarial_hotpotqa':\"validation\"}\n",
    "    res = []\n",
    "    \n",
    "    for dataset_str in test_datasets:\n",
    "        target_split = dataset2split[dataset_str] #\"validation_mismatched\" if dataset_str == 'multi_nli' else \"test\"\n",
    "        dataset = load_dataset(dataset_str, split=target_split)\n",
    "\n",
    "        if dataset_str in ['sagnikrayc/snli-bt','sagnikrayc/snli-cf-kaushik']: dataset = dataset.map(convertlabels2ids) \n",
    "\n",
    "        tokenized_test_dataset = dataset.map(tokenize_function, batched=True, num_proc=num_proc).filter(lambda sample: sample['label'] in list(range(num_labels)))\n",
    "\n",
    "        results = trainer.evaluate(tokenized_test_dataset)\n",
    "        res.append([model_name, dataset_str,results['eval_accuracy']])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac24d58",
   "metadata": {},
   "source": [
    "# Testing Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90086aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import adapters\n",
    "from adapters import AdapterTrainer\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def human_format(num):\n",
    "    num = float('{:.3g}'.format(num))\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '{}{}'.format('{:f}'.format(num).rstrip('0').rstrip('.'), ['', 'K', 'M', 'B', 'T'][magnitude])\n",
    "\n",
    "def build_dataset(tokenizer):\n",
    "    \n",
    "    def filter_function(examples):\n",
    "        question = examples[\"question\"]\n",
    "        context = examples[\"context\"]\n",
    "        input_len = len(tokenizer(question,context, truncation=False).input_ids)\n",
    "        return (input_len < tokenizer.model_max_length)\n",
    "    \n",
    "    def tokenize_train(examples):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "        answers = examples[\"answers\"]\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            answer = answers[i]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "        return inputs\n",
    "    \n",
    "    dataset = load_dataset(\"squad\", split=\"train\")\n",
    "    dataset = dataset.filter(filter_function)\n",
    "    train_dataset = dataset.map(tokenize_train, batched=True, num_proc=4, remove_columns=dataset.column_names)\n",
    "    return train_dataset\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples, metric):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "    \n",
    "    n_best = 20\n",
    "    max_answer_length = 30\n",
    "    predicted_answers = []\n",
    "    \n",
    "    metric = evaluate.load(\"squad\")\n",
    "    \n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "def evaluate_test_sets(tokenizer, trainer, model_name):\n",
    "    def newsqa2squad(sample):\n",
    "        answers_text = sample[\"answers\"]\n",
    "        answers_start = sample[\"labels\"][0][\"start\"]\n",
    "        sample[\"answers\"] = {'text':answers_text, 'answer_start':answers_start}\n",
    "        return sample\n",
    "    \n",
    "    def filter_out_yes_no_answers(sample):\n",
    "        return sample['answer'].lower() != 'yes' and sample['answer'].lower() != 'no'\n",
    "\n",
    "    def advhotpotqa2squad(sample):\n",
    "        supporting_facts = sample['supporting_facts']\n",
    "        context_texts = sample['context']\n",
    "        titles_to_sentences = collections.defaultdict(list)\n",
    "\n",
    "        for title,sentence in zip(context_texts['title'],context_texts['sentences']):\n",
    "            for s in sentence:\n",
    "                text = s.strip()\n",
    "                titles_to_sentences[title].append(text)\n",
    "\n",
    "        sf_titles = supporting_facts['title']\n",
    "        sf_sent_id = supporting_facts['sent_id']\n",
    "\n",
    "        sf_context=[]\n",
    "        for i,sf_title in enumerate(sf_titles):\n",
    "            try:\n",
    "                text = titles_to_sentences[sf_title][sf_sent_id[i]]\n",
    "            except:\n",
    "                sample['_id'] = 'skip'\n",
    "            sf_context.append(text.strip())\n",
    "\n",
    "        sample['id'] = sample['_id']\n",
    "        # ---Shuffle sentences in context and join them into a single string ---#\n",
    "        context_list = sf_context + titles_to_sentences['added']\n",
    "        random.shuffle(context_list)\n",
    "        sample['context'] = ' '.join(context_list)\n",
    "        sample['answers'] = {'text':[sample['answer']], 'answer_start':[sample['context'].find(sample['answer'])]}\n",
    "        return sample\n",
    "\n",
    "    def filter_function(examples):\n",
    "        question = examples[\"question\"]\n",
    "        context = examples[\"context\"]\n",
    "        input_len = len(tokenizer(question,context, truncation=False).input_ids)\n",
    "        return (input_len < tokenizer.model_max_length)\n",
    "\n",
    "    def tokenize(examples):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            return_offsets_mapping=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "        example_ids = []\n",
    "        for i in range(len(inputs[\"input_ids\"])):\n",
    "            sample_idx = sample_map[i]\n",
    "            example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "            offset = inputs[\"offset_mapping\"][i]\n",
    "            inputs[\"offset_mapping\"][i] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "        inputs[\"example_id\"] = example_ids\n",
    "        return inputs\n",
    "    \n",
    "    test_datasets = ['squad','lucadiliello/newsqa','sagnikrayc/adversarial_hotpotqa'] #\n",
    "    dataset2split = {'squad':\"validation\", 'lucadiliello/newsqa':\"validation\", 'sagnikrayc/adversarial_hotpotqa':\"validation\"}\n",
    "    res = []\n",
    "    metric = evaluate.load(\"squad\")\n",
    "    \n",
    "    for dataset_str in test_datasets:\n",
    "        target_split = dataset2split[dataset_str] \n",
    "        dataset = load_dataset(dataset_str, split=target_split)\n",
    "        \n",
    "        if dataset_str == 'lucadiliello/newsqa': dataset = dataset.map(newsqa2squad, num_proc=4).rename_column(\"key\",\"id\")\n",
    "        \n",
    "        if dataset_str == 'sagnikrayc/adversarial_hotpotqa':\n",
    "            dataset = dataset.filter(filter_out_yes_no_answers, num_proc=4) \n",
    "            dataset = dataset.map(advhotpotqa2squad, num_proc=4, remove_columns=[\"type\",'level','supporting_facts','answer','_id']).filter(lambda x: x['id']!='skip')\n",
    "\n",
    "        dataset = dataset.filter(filter_function)\n",
    "        \n",
    "        tokenized_dataset = dataset.map(tokenize, batched=True, num_proc=4,remove_columns=dataset.column_names)\n",
    "\n",
    "        predictions, _, _ = trainer.predict(tokenized_dataset)\n",
    "        start_logits, end_logits = predictions\n",
    "        results = compute_metrics(start_logits, end_logits, tokenized_dataset, dataset, metric)\n",
    "        res.append([model_name, dataset_str,results['f1']])\n",
    "    return res\n",
    "\n",
    "def log_and_save_results(res,\n",
    "    results_dir = '../../result_logs',\n",
    "    outfile_name = 'squad_finetuning_performances.csv'\n",
    "):\n",
    "    outfile_path = os.path.join(results_dir, outfile_name)\n",
    "\n",
    "    if not os.path.exists(results_dir): os.mkdir(results_dir)\n",
    "\n",
    "    if not os.path.exists(outfile_path):\n",
    "        with open(outfile_path,'a', newline='\\n') as f:\n",
    "            f.write(\"date_time; model_name; dataset; f1_score\\n\")\n",
    "\n",
    "    today = datetime.today()\n",
    "\n",
    "    for i  in res:\n",
    "        model_name, dataset_str, accuracy = i\n",
    "        with open(outfile_path,'a', newline='\\n') as f:\n",
    "            f.write(f\"{today}; {model_name}; {dataset_str}; {accuracy}\\n\")\n",
    "        print(f\"F1 score of {model_name} on {dataset_str} dataset: {accuracy}\")\n",
    "\n",
    "def main(\n",
    "    model_checkpoint,\n",
    "    seed: int=42,\n",
    "    batch_size: int=64,\n",
    "    num_train_epochs: int=3,\n",
    "    num_proc: int=4,\n",
    "    output_dir: str=\"../../result_logs\",\n",
    "    do_train: bool=True,\n",
    "    do_eval: bool=True,\n",
    "    do_log: bool=True,\n",
    "    use_adapter: bool=False,\n",
    "    use_peft: bool=False,\n",
    "    save_path: str=\"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/nlp-gen\" \n",
    "):\n",
    "    \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    checkpoint = model_checkpoint\n",
    "    model_name = checkpoint.split(\"/\")[-1]\n",
    "    \n",
    "    #tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    #model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length = model.config.max_position_embeddings) # Need to mention model_max_length mainly for OPT models\n",
    "    \n",
    "    train_dataset = build_dataset(tokenizer)\n",
    "    \n",
    "    data_collator = DefaultDataCollator()\n",
    "    \n",
    "    # LOAD ADAPTER\n",
    "    if use_adapter:\n",
    "        print(\"Initializing Adapters for transformer model\")\n",
    "        adapters.init(model)\n",
    "        model.add_adapter(\"squad\", config=\"seq_bn\")\n",
    "        model.train_adapter(\"squad\")\n",
    "        # print number of trainable parameters\n",
    "        summary = model.adapter_summary(as_dict=True)\n",
    "        print(f\"trainable params: {summary[0]['#param']:,d} || all params: {summary[1]['#param']:,d} || trainable%: {summary[0]['%param']}\")\n",
    "        # edit model name\n",
    "        num_param = human_format(summary[0]['#param'])\n",
    "        model_name = f\"ADAPTER/{model_name}-bn-adapter-{num_param}\"\n",
    "    \n",
    "    # LOAD PEFT MODEL\n",
    "    if use_peft:\n",
    "        print(\"Loading PEFT(LORA) Model\")\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.QUESTION_ANS,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        # print number of trainable paramaeters\n",
    "        model.print_trainable_parameters()\n",
    "        # edit model name\n",
    "        num_param = human_format(model.get_nb_trainable_parameters()[0])\n",
    "        model_name = f\"PEFT/{model_name}-lora-{num_param}\"\n",
    "    \n",
    "    save_path = f\"{save_path}/{model_name}-squad\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_path,\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit=1,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,\n",
    "        #push_to_hub=True,\n",
    "        overwrite_output_dir=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    if do_train:\n",
    "        trainer.train()\n",
    "    \n",
    "    # EVALUATE PERFORMANCE ON TEST SETS   \n",
    "    if do_eval:\n",
    "        results = evaluate_test_sets(tokenizer, trainer, model_name)\n",
    "        # LOG RESULT METRICS\n",
    "        if do_log:\n",
    "            log_and_save_results(results, results_dir = output_dir, outfile_name = 'squad_finetuning_performances.csv')\n",
    "        else:\n",
    "            print(results)\n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "#    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a06e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='4101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 546/4101 07:24 < 48:22, 1.22 it/s, Epoch 0.40/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.941300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(model_checkpoint=\"bert-base-cased\", do_log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f5539b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PEFT(LORA) Model\n",
      "trainable params: 592,900 || all params: 108,312,580 || trainable%: 0.5473971721475013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4101' max='4101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4101/4101 57:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.481900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17f085eb0fb4c06864b667785c8e62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10511 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5162b75809974e9592a6893bc2a0046e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a0f4687998489ea14ea6b1e2126bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f39177ee57748c38bb2119774068735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3abe6dca35f43c4ae77cd5a734b7ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c14327e8404dbfa43854db50863849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PEFT/bert-base-cased-lora-593K', 'squad', 51.60822727415095], ['PEFT/bert-base-cased-lora-593K', 'lucadiliello/newsqa', 23.115242358244668], ['PEFT/bert-base-cased-lora-593K', 'sagnikrayc/adversarial_hotpotqa', 6.474574607811869]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(model_checkpoint=\"bert-base-cased\", do_log=False, use_peft=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623d4635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de442ac26a648f395c5e8c435eca07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896b283985564f71ac1eb556d38a1304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/87459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Adapters for transformer model\n",
      "trainable params: 894,528 || all params: 107,719,680 || trainable%: 0.830422073292457\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2734' max='2734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2734/2734 17:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.138200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f95b5a163144fc5ae6090f62fac26df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baaaddf705745f48a9f62a8f2a57252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10511 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5409fa7c2c26434e871b02edd2243e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c98c6d43fae4ec8b56a05d5aae2c764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ff6a3c842d406b9785787c5f91bfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af6e16f34454eb1a4cc9f3c7fd72215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c397a6c210934e16998595430b9bc698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8fbb96cbb9491d99851cf5b63ce93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17035bf45fe14f19b2578c22bb63d672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ADAPTER/bert-base-cased-bn-adapter-895K', 'squad', 60.648934239742346], ['ADAPTER/bert-base-cased-bn-adapter-895K', 'lucadiliello/newsqa', 35.29609138620631], ['ADAPTER/bert-base-cased-bn-adapter-895K', 'sagnikrayc/adversarial_hotpotqa', 11.004445344388099]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(model_checkpoint=\"bert-base-cased\", batch_size=32, num_train_epochs=1, do_log=False, use_adapter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d12104",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/run_qa.py --model_checkpoint \"bert-large-cased\" --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27d4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ../scripts/run_qa.py --model_checkpoint \"roberta-base\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
