{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f0cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varu/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "from trainer_seq2seq_qa import QuestionAnsweringSeq2SeqTrainer\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c15507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d29156a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330e632a499b42479c4ac89ce8d1bcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_function(examples):\n",
    "    question = examples[\"question\"]\n",
    "    context = examples[\"context\"]\n",
    "    input_len = len(tokenizer(question,context, truncation=False).input_ids)\n",
    "    return (input_len < tokenizer.model_max_length)\n",
    "\n",
    "dataset = dataset.filter(filter_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0818284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saint Bernadette Soubirous'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"answers\"]['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd52a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_squad_batch(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    def generate_input(_question, _context):\n",
    "        return \" \".join([\"question:\", _question.lstrip(), \"context:\", _context.lstrip()])\n",
    "\n",
    "    inputs = [generate_input(question, context) for question, context in zip(questions, contexts)]\n",
    "    targets = [answer[\"text\"][0] if len(answer[\"text\"]) > 0 else \"\" for answer in answers]\n",
    "    return inputs, targets\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs, targets = preprocess_squad_batch(examples)\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(text_target=targets, max_length=30, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5885001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b255026c72604f0cb31ee7d1267d1d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/87310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f20903",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad_v2\" if data_args.version_2_with_negative else \"squad\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "# Post-processing:\n",
    "def post_processing_function(\n",
    "    examples: datasets.Dataset, features: datasets.Dataset, outputs: EvalLoopOutput, stage=\"eval\"\n",
    "):\n",
    "    # Decode the predicted tokens.\n",
    "    preds = outputs.predictions\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    feature_per_example = {example_id_to_index[feature[\"example_id\"]]: i for i, feature in enumerate(features)}\n",
    "    predictions = {}\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(examples):\n",
    "        # This is the index of the feature associated to the current example.\n",
    "        feature_index = feature_per_example[example_index]\n",
    "        predictions[example[\"id\"]] = decoded_preds[feature_index]\n",
    "\n",
    "    # Format the result to the format the metric expects.\n",
    "    if data_args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path: str=\"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/nlp-gen/qa\"\n",
    "name = model_name.split(\"/\")[-1]\n",
    "save_path = f\"{save_path}/{name}-squad\"\n",
    "batch_size = 32\n",
    "num_train_epochs = 1\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy = \"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = QuestionAnsweringSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=post_processing_function,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
