{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e23ac9",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a376c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "id2label = {0:'entailment', 1:'neutral', 2:'contradiction'}\n",
    "label2id = {'entailment':0, 'neutral':1, 'contradiction':2}\n",
    "num_labels = len(id2label)\n",
    "max_target_length = 5\n",
    "\n",
    "def preprocess_snli_batch(examples):\n",
    "    premises = examples['premise']\n",
    "    hypotheses = examples['hypothesis']\n",
    "    labels = examples['label']\n",
    "\n",
    "    def generate_input(_premise, _hypothesis):\n",
    "        return \" \".join([\"premise:\", _premise, \"hypothesis:\", _hypothesis])\n",
    "\n",
    "    inputs = [generate_input(premise, hypothesis) for premise, hypothesis in zip(premises, hypotheses)]\n",
    "    targets = [id2label[label] if (label) in range(num_labels) else \"\" for label in labels]\n",
    "    return inputs, targets\n",
    "\n",
    "def convertlabels2ids(example):\n",
    "    example['label'] = label2id[example['label']]\n",
    "    return example\n",
    "    \n",
    "\n",
    "def log_and_save_results(\n",
    "    res,\n",
    "    results_dir = \"../res\",\n",
    "    outfile_name = \"snli_model_performances.csv\"\n",
    "):\n",
    "    outfile_path = os.path.join(results_dir, outfile_name)\n",
    "\n",
    "    if not os.path.exists(results_dir): os.mkdir(results_dir)\n",
    "\n",
    "    if not os.path.exists(outfile_path):\n",
    "        with open(outfile_path,'a', newline='\\n') as f:\n",
    "            f.write(\"date; model_name; dataset; accuracy\\n\")\n",
    "\n",
    "    today = date.today()\n",
    "\n",
    "    for i  in res:\n",
    "        model_name, dataset_str, accuracy = i\n",
    "        with open(outfile_path,'a', newline='\\n') as f:\n",
    "            f.write(f\"{today};{model_name}; {dataset_str}; {accuracy}\\n\")\n",
    "        print(f\"Accuracy of {model_name} on {dataset_str} dataset: {accuracy}\")\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_checkpoint,\n",
    "    seed: int=42,\n",
    "    batch_size: int=64,\n",
    "    num_train_epochs: int= 3,\n",
    "    num_proc: int=4,\n",
    "    max_train_samples=None,\n",
    "    max_eval_samples=None,\n",
    "    output_dir: str=\"../res\",\n",
    "    use_peft: bool = False,\n",
    "    do_train: bool = True,\n",
    "    do_eval: bool=True,\n",
    "    do_log: bool=True,\n",
    "    save_path: str=\"/nfs/turbo/umms-vgvinodv/models/finetuned-checkpoints/nlp-gen\"\n",
    "):\n",
    "    # Set Seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    checkpoint = model_checkpoint\n",
    "    metric_name = \"accuracy\"\n",
    "    model_name = checkpoint.split(\"/\")[-1]\n",
    "    save_path = f\"{save_path}/{model_name}-snli\"\n",
    "    \n",
    "    # Load Dataset\n",
    "    raw_dataset = load_dataset(\"snli\")\n",
    "    raw_dataset = raw_dataset.filter(lambda sample: sample['label'] in id2label)\n",
    "    \n",
    "    # Load Model and Tokenizer\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs, targets = preprocess_snli_batch(examples)\n",
    "        model_inputs = tokenizer(inputs)\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "        predictions, labels = eval_pred\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        pred_ids = [label2id[p] if p in label2id else -1 for p in decoded_preds]\n",
    "\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        label_ids = [label2id[l] if l in label2id else -1 for l in decoded_labels]\n",
    "\n",
    "        result = metric.compute(predictions=pred_ids, references=label_ids) \n",
    "        return result\n",
    "    \n",
    "    # Tokenize raw dataset\n",
    "    column_names = raw_dataset['train'].column_names\n",
    "    train_dataset = raw_dataset[\"train\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=column_names,\n",
    "    )\n",
    "    if max_train_samples is not None:\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    \n",
    "    eval_dataset = raw_dataset[\"validation\"].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=column_names,\n",
    "    )\n",
    "    if max_eval_samples is not None:\n",
    "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer,model=model,)\n",
    "    \n",
    "    # Training Args\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        save_path,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=metric_name,\n",
    "        overwrite_output_dir=True,\n",
    "        #push_to_hub=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize our Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    trainer.train()  \n",
    "    \n",
    "    # HELPER_FUNC\n",
    "    def evaluate_test_data():\n",
    "        test_datasets = ['snli','multi_nli','sagnikrayc/snli-bt','sagnikrayc/snli-cf-kaushik']\n",
    "        dataset2split = {'snli':\"test\", 'multi_nli':\"validation_mismatched\", 'sagnikrayc/snli-bt':\"test\", 'sagnikrayc/snli-cf-kaushik':\"test\"}\n",
    "        res = []\n",
    "\n",
    "        for dataset_str in test_datasets:\n",
    "            target_split = dataset2split[dataset_str]#\"validation_mismatched\" if dataset_str == 'multi_nli' else \"test\"\n",
    "            dataset = load_dataset(dataset_str, split=target_split)\n",
    "            \n",
    "            if dataset_str in ['sagnikrayc/snli-bt','sagnikrayc/snli-cf-kaushik']: dataset = dataset.map(convertlabels2ids) \n",
    "            dataset = dataset.filter(lambda sample: sample['label'] in list(range(num_labels)))\n",
    "            \n",
    "            tokenized_test_dataset = dataset.map(preprocess_function, batched=True, num_proc=num_proc, remove_columns=dataset.column_names,)\n",
    "            \n",
    "            results = trainer.evaluate(tokenized_test_dataset)\n",
    "            res.append([model_name, dataset_str,results['eval_accuracy']])\n",
    "        return res\n",
    "    \n",
    "    # Compute performance on test data\n",
    "    res = evaluate_test_data()\n",
    "    \n",
    "    # Save results to CSV file\n",
    "    if do_log:\n",
    "        log_and_save_results(res, results_dir = output_dir, outfile_name = 'snli_model_performances.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf9f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(model_checkpoint=\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(model_checkpoint=\"t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(model_checkpoint=\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(model_checkpoint=\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0b951",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f146bb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e37862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06db10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "#def convertid2label(example):\n",
    "#    example['label_str'] = id2label[example['label']]\n",
    "#    return example\n",
    "\n",
    "id2label = {0:'entailment', 1:'neutral', 2:'contradiction'}\n",
    "\n",
    "raw_dataset = load_dataset(\"snli\")\n",
    "raw_dataset = raw_dataset.filter(lambda sample: sample['label'] in id2label)#.map(convertid2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3fe9a",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46485b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google/flan-t5-base\"\n",
    "num_labels = 3\n",
    "max_train_samples = 100\n",
    "#max_train_samples = None\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_length = 5\n",
    "#padding = \"max_length\"\n",
    "#ignore_pad_token_for_loss = True\n",
    "\n",
    "def preprocess_snli_batch(examples):\n",
    "    premises = examples['premise']\n",
    "    hypotheses = examples['hypothesis']\n",
    "    labels = examples['label']\n",
    "\n",
    "    def generate_input(_premise, _hypothesis):\n",
    "        return \" \".join([\"premise:\", _premise, \"hypothesis:\", _hypothesis])\n",
    "\n",
    "    inputs = [generate_input(premise, hypothesis) for premise, hypothesis in zip(premises, hypotheses)]\n",
    "    targets = [id2label[label] if (label) in range(num_labels) else \"\" for label in labels]\n",
    "    return inputs, targets\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs, targets = preprocess_snli_batch(examples)\n",
    "\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    # Setup the tokenizer for targets \n",
    "    #with tokenizer.as_target_tokenizer():\n",
    "    #    labels = tokenizer(targets, max_length=5, padding=padding)\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    #if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
    "    #    labels[\"input_ids\"] = [\n",
    "    #        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    #    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    label2id = {'entailment':0,'neutral':1,'contradiction':2}\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    pred_ids = [label2id[p] if p in label2id else -1 for p in decoded_preds]\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    label_ids = [label2id[l] if l in label2id else -1 for l in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=pred_ids, references=label_ids)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaac395",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "column_names = raw_dataset[\"train\"].column_names\n",
    "\n",
    "if max_train_samples is not None:\n",
    "    # We will select sample from whole data if agument is specified\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    \n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=column_names,\n",
    "    #load_from_cache_file=not data_args.overwrite_cache,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset = raw_dataset[\"validation\"]\n",
    "column_names = raw_dataset[\"validation\"].column_names  \n",
    "if max_train_samples is not None:\n",
    "    # We will select sample from whole data if agument is specified\n",
    "    eval_dataset = eval_dataset.select(range(max_train_samples))\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=column_names,\n",
    "    #load_from_cache_file=not data_args.overwrite_cache,\n",
    "    desc=\"Running tokenizer on eval dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52589e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "#label_pad_token_id = -100 if ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    #label_pad_token_id=label_pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b23783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Args\n",
    "metric_name = \"accuracy\"\n",
    "batch_size = 64\n",
    "num_train_epochs = 1\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"finetuned-checkpoints/{model_name}-snli\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    #load_best_model_at_end=True,\n",
    "    #metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b236492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(f\"{model_name}-snli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc6314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'premise: A person on a horse jumps over a broken down airplane. hypothesis: A person is at a diner, ordering an omelette.'\n",
    "inputs = tokenizer.encode_plus(text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
    "outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=100)\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cffbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_dataset[0]\n",
    "outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=100)\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21419c07",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertids2labels(example):\n",
    "    example['label'] = ids2label[example['label']]\n",
    "    return example\n",
    "\n",
    "ids2label = {'entailment':0, 'neutral':1, 'contradiction':2}\n",
    "\n",
    "test_datasets = ['snli','multi_nli','sagnikrayc/snli-bt','sagnikrayc/snli-cf-kaushik']\n",
    "res = []\n",
    "\n",
    "for dataset_str in test_datasets:\n",
    "    target_split = \"validation_mismatched\" if dataset_str == 'multi_nli' else \"test\"\n",
    "    dataset = load_dataset(dataset_str, split=target_split)\n",
    "    if dataset_str in ['sagnikrayc/snli-bt','sagnikrayc/snli-cf-kaushik']: dataset = dataset.map(convertids2labels) \n",
    "    dataset = dataset.filter(lambda sample: sample['label'] in list(range(num_labels)))\n",
    "    tokenized_test_dataset = dataset.map(preprocess_function,batched=True,remove_columns=column_names,)\n",
    "    results = trainer.evaluate(tokenized_test_dataset)\n",
    "    res.append([model_name, dataset_str,results['eval_accuracy']])\n",
    "    #print(f\"Accuracy of {model_name} on {dataset_str} dataset: {results['eval_accuracy']}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6962b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3743f",
   "metadata": {},
   "source": [
    "# Log and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40100891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "results_dir = 'res'\n",
    "outfile_name = 'snli_model_performances.csv'\n",
    "\n",
    "outfile_path = os.path.join(results_dir, outfile_name)\n",
    "\n",
    "if not os.path.exists(results_dir): os.mkdir(results_dir)\n",
    "\n",
    "if not os.path.exists(outfile_path):\n",
    "    with open(outfile_path,'a', newline='\\n') as f:\n",
    "        f.write(\"date; model_name; dataset; accuracy\\n\")\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "for i  in res:\n",
    "    model_name, dataset_str, accuracy = i\n",
    "    with open(outfile_path,'a', newline='\\n') as f:\n",
    "        f.write(f\"{today};{model_name}; {dataset_str}; {accuracy}\\n\")\n",
    "    print(f\"Accuracy of {model_name} on {dataset_str} dataset: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479caea",
   "metadata": {},
   "source": [
    "## Testing T5forSeqClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a524066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datasets import load_dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"snli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Train dataset size: 550152\n",
    "# Test dataset size: 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98034353",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    def generate_input(_premise, _hypothesis):\n",
    "        return \" \".join([\"premise:\", _premise, \"hypothesis:\", _hypothesis])\n",
    "    sample[\"text\"] = [generate_input(_premise,_hypothesis) for _premise,_hypothesis in zip(sample['premise'],sample['hypothesis'])]\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True, num_proc=4, remove_columns=['premise','hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b71cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f917e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f404c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(samples):\n",
    "    model_inputs = tokenizer(samples[\"text\"])\n",
    "\n",
    "    labels = tokenizer(text_target=samples[\"label\"], max_length=5, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode('0'))\n",
    "print(tokenizer.encode('1'))\n",
    "print(tokenizer.encode('2'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
